{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "The purpose of this notebook is to do prospection on data obtained from scriping interests links and Zefix results.\n",
    "\n",
    "Indeed, on issue that may arise is that the data aren't clean enough to find usable results. In other words, we can have this two type of errors:\n",
    "\n",
    "- False positive: for example, we search for a company linked with a politician, and we obtain on a lot of different companies\n",
    "- False negatives: data isn't clean and thus can't find the real company in Zefix\n",
    "\n",
    "This notebook is trying to have an insight on this two issues.\n",
    "\n",
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from zefix_scraper import zefix_search_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_findings(name):\n",
    "    \"\"\"\n",
    "    Count how many findings zefix has done.\n",
    "    \"\"\"\n",
    "    page = zefix_search_raw(name)\n",
    "    \n",
    "    if page is None:\n",
    "        return 0\n",
    "\n",
    "    content = BeautifulSoup(page, 'lxml')\n",
    "    \n",
    "    return len(content.body.find_all('p')) -1\n",
    "\n",
    "def cached_call(generator, filename, as_series=False):\n",
    "    \"\"\"\n",
    "    Simple function that try to load from cache or generate data (and cache it)\n",
    "    \n",
    "    `generator` must returns a DataFrame (and not a Series) in order to simplify the work.\n",
    "    \"\"\"\n",
    "    path = os.path.join('cache', \"{}.json\".format(filename))\n",
    "    try:\n",
    "        if as_series:\n",
    "            ans = pd.read_json(path, typ='series', orient='records')\n",
    "        else:\n",
    "            ans = pd.read_json(path)\n",
    "    except Exception as e:\n",
    "        print(\"Loading data... ({})\".format(e))\n",
    "        ans = generator()\n",
    "        ans.to_json(path)\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interests = pd.read_json('data/all_interests.json')\n",
    "interests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "all_interests = interests.interest_name.unique()\n",
    "\n",
    "def lookup_interests(all_interests):\n",
    "    \"\"\"\n",
    "    Propagates Zefix lookups asynchronously usint a thread pool.\n",
    "    \n",
    "    The argument is simply a list, and the result is a DataFrame containing the interest as \n",
    "    a key and a value unique field called `findings_count`. \n",
    "    \n",
    "    This method should be refactored before being used elsewhere because:\n",
    "    - Not clean to have a simple list in argument, should use Pandas' Series\n",
    "    - Not clean to have a DataFrame in output, should have a serie which maintains the same index\n",
    "    \"\"\"\n",
    "    all_interests = map(lambda x: x.strip(), all_interests)\n",
    "    all_interest_count = {}\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures_data = {executor.submit(count_findings, interest): interest for interest in all_interests}\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures_data):\n",
    "            interest = futures_data[future]\n",
    "            try:\n",
    "                count = future.result()\n",
    "            except Exception as e:\n",
    "                print(\"{} generated exception: {}\".format(interest, e))\n",
    "            else: \n",
    "                all_interest_count[interest] = count\n",
    "            \n",
    "    return pd.DataFrame(all_interest_count, index=[\"findings_count\"]).transpose()\n",
    "\n",
    "found_interests = cached_call(lambda : lookup_interests(all_interests), 'interests_counts')\n",
    "found_interests = found_interests.findings_count\n",
    "found_interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "found_interests.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "found_interests.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def describe_ratios(serie):\n",
    "    data = [\n",
    "        (\"equal_one\", (serie == 1).mean()),\n",
    "        (\"null\", (serie == 0).mean()),\n",
    "        (\"more_one\", (serie > 1).mean()),\n",
    "    ]\n",
    "    \n",
    "    for k,v in data:\n",
    "        print(\"{}: {:0.1f}%\".format(k,v * 100))\n",
    "\n",
    "describe_ratios(found_interests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the vast majority of Zefix lookups have no result (roughly 75%), while 8% have more than one results.\n",
    "\n",
    "On the 75% of lookups having no results, some are true negative (i.e. the legal entity isn't registered in the Commercial Register, which is totally valid on some cases). However, 85% looks to be a lot too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve more lookups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let have a look to the distribution of terms in the interest list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [val for sublist in l for val in sublist]\n",
    "\n",
    "def strip_flatten(l):\n",
    "    return [e.strip() for e in flatten(l)]\n",
    "\n",
    "all_words = pd.Series(strip_flatten([interest.split() for interest in all_interests]))\n",
    "all_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_words = all_words.groupby(all_words).count()\n",
    "grouped_words.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "describe_ratios(grouped_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_cloud(words):\n",
    "    cloud = WordCloud().generate(' '.join(words))\n",
    "    plt.imshow(cloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "show_cloud(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ordered = grouped_words.sort_values(ascending=False).reset_index(drop=True)\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "ordered.plot(ax=axs[0])\n",
    "ordered.apply(np.log).plot(ax=axs[1])\n",
    "\n",
    "axs[0].set_title(\"Linear scale\")\n",
    "axs[1].set_title(\"Log scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is _very few very frequent_ words, and a lot of non-frequent word. In this case - rather than on standard information retrieval â€“ low frequence words might be worth considering, since companies have precise names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let check parenthesis in the data (which might be a problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interests[interests.interest_name.str.contains('\\(')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, all of the content between the two parenthesis is irrelevant for the Zefix search (being either status or acronyms for the company). In both cases, this might result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_parenthesis(text):\n",
    "    return re.sub(\"\\(.*\\)?\",'', text).strip()\n",
    "\n",
    "clean_interests = interests.interest_name.apply(remove_parenthesis)\n",
    "clean_interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download()        # run once\n",
    "\n",
    "def stringify_list(l):\n",
    "    return ' '.join(l)\n",
    "\n",
    "def tokenize_interest_pipeline(interest):\n",
    "    def tokenize(interest):\n",
    "        return nltk.word_tokenize(interest)\n",
    "    \n",
    "    def remove_stopwords(sentence):\n",
    "        stop_words = stopwords.words(['german', 'french'])\n",
    "        return [w for w in sentence if w.lower() not in stop_words]\n",
    "    \n",
    "    ans = remove_parenthesis(interest)\n",
    "    ans = tokenize(ans)\n",
    "    ans = remove_stopwords(ans)\n",
    "    \n",
    "    return ans\n",
    "\n",
    "tokenized_interests = list(set(flatten([tokenize_interest_pipeline(i) for i in all_interests])))\n",
    "sorted(tokenized_interests, key=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see, there is token of size 1 which doens't provide a lot of information. Thus, we update the pipeline in order to remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def more_one_letter_pipeline(interest):\n",
    "    ans = tokenize_interest_pipeline(interest)\n",
    "    \n",
    "    return [token for token in ans if len(token) > 1]\n",
    "\n",
    "tokenized_interests = flatten([more_one_letter_pipeline(i) for i in all_interests])\n",
    "show_cloud(tokenized_interests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, _AG_ is the most frequent word in the list now. It's an issue since it might provide enormously false-positive on Zefix search engine (due to its small size). Therefore, AG and SA are dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dark_words = ['AG','SA']\n",
    "\n",
    "def remove_dark_words_pipeline(interest):\n",
    "    return [token for token in more_one_letter_pipeline(interest) if token.upper() not in dark_words]\n",
    "\n",
    "tokenized_interests = flatten([remove_dark_words_pipeline(i) for i in all_interests])\n",
    "show_cloud(tokenized_interests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanitized search\n",
    "Now that we've a pipeline for sanitizing interests, let do the lookup on Zefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_pipeline = remove_dark_words_pipeline\n",
    "string_pipeline = lambda s: stringify_list(final_pipeline(s))\n",
    "\n",
    "interests['sanitized_interest'] = interests.interest_name.apply(string_pipeline)\n",
    "interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sanitzed_interests = interests.sanitized_interest.unique()\n",
    "found_sanitized_interests = cached_call(lambda : lookup_interests(all_sanitzed_interests), 'analyze_weak_lookup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "found_sanitized_interests.sort_values('findings_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "describe_ratios(found_sanitized_interests.findings_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Mixing all together\n",
    "Now that we've two methods that works differently, we should take advantage of both ones to have better results from Zefix.\n",
    "\n",
    "The idea is to first look for unique result using an explicit lookup, and then to change the name of the interest for the other cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a new method for looking up Zefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def async_series_lookup(f, input_series, number_parallel_tasks=None):\n",
    "    \"\"\"\n",
    "    Asynchronous lookup for Pandas Series\n",
    "    \n",
    "    The argument is simply a list, and the result is a DataFrame containing the interest as \n",
    "    a key and a value unique field called `findings_count`. \n",
    "    \n",
    "    Output: f(input_series) asynchronously\n",
    "    \n",
    "    f -- function to apply\n",
    "    input_series -- Series to apply the function on\n",
    "    \"\"\"\n",
    "    results = pd.Series()\n",
    "    \n",
    "    if number_parallel_tasks is not None:\n",
    "        get_executor = lambda: concurrent.futures.ThreadPoolExecutor(max_workers=number_parallel_tasks)\n",
    "    else:\n",
    "        get_executor = lambda: concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    with get_executor() as executor:\n",
    "        futures_data = {executor.submit(f, val): key for (key, val) in input_series.iteritems()}\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures_data):\n",
    "            key = futures_data[future]\n",
    "            try:\n",
    "                ans = future.result()\n",
    "            except Exception as e:\n",
    "                print(\"{} generated exception: {}\".format(key, e))\n",
    "            else: \n",
    "                results.set_value(key, ans)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've this helper function, let call it on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resolved_interests = pd.DataFrame(all_interests, columns=['fullname'])\n",
    "resolved_interests['sanitized_interest'] = resolved_interests.fullname.apply(string_pipeline)\n",
    "resolved_interests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong lookup\n",
    "Lookup the fullname entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resolved_interests['strong_lookup_counts'] = cached_call(lambda : async_series_lookup(count_findings, resolved_interests.fullname, number_parallel_tasks=1), 'strong_lookup_resolved', True)\n",
    "resolved_interests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weak_lookup(resolved_interests):\n",
    "    interests_to_look = resolved_interests[resolved_interests.strong_lookup_counts != 1].sanitized_interest\n",
    "    ans = resolved_interests.strong_lookup_counts.copy()\n",
    "    ans.update(async_series_lookup(count_findings, interests_to_look, number_parallel_tasks=1))\n",
    "    \n",
    "    return ans\n",
    "\n",
    "resolved_interests['grouped_lookup_counts'] = cached_call(lambda: weak_lookup(resolved_interests), 'weak_lookup_resolved', True)\n",
    "resolved_interests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "describe_ratios(resolved_interests.grouped_lookup_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:adaenv]",
   "language": "python",
   "name": "conda-env-adaenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
